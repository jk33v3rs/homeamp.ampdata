# Drift Detection: "Odd One Out" Analysis

**Goal**: Identify instances where ONE instance has a different setting than all the others

**Example**: 19 instances have `CMI.ReSpawn.Enabled = true`, but DEV01 has `false` → Flag as outlier

---

## Detection Strategy

### Method 1: Statistical Outlier Detection

For each plugin setting across all instances:
1. Count occurrences of each unique value
2. Identify the **majority value** (most common)
3. Flag instances with **minority values** as outliers

**Example:**
```
Plugin: CMI
Setting: ReSpawn.Enabled
Values:
  - true: 19 instances (SMP201, HUB01, CLIP01, ...)
  - false: 1 instance (DEV01)

Result: DEV01 flagged as outlier
```

### Method 2: Deviation from Universal Config

Compare each instance against universal baseline:
1. Load universal config (should be identical across all)
2. For each instance, find settings that differ from universal
3. Check if that deviation is **unique** (only 1-2 instances have it)

**Example:**
```
Universal: bStats.serverUuid = <auto-generated>
Expected: Each instance has unique UUID (not drift)

Universal: CMI.ReSpawn.Enabled = true
SMP201: true ✅
HUB01: true ✅
DEV01: false ⚠️ OUTLIER (intentional for testing?)
```

---

## Algorithm: Outlier Scoring

```python
def calculate_outlier_score(plugin_name: str, setting_key: str, instances: dict) -> dict:
    """
    Calculate outlier score for a setting across instances.
    
    Returns dict of {instance_name: outlier_score}
    Higher score = more unusual
    """
    # Count value occurrences
    value_counts = {}
    for instance, value in instances.items():
        if value not in value_counts:
            value_counts[value] = []
        value_counts[value].append(instance)
    
    # Calculate total instances
    total = len(instances)
    
    # Assign outlier scores (inverse of frequency)
    scores = {}
    for value, instance_list in value_counts.items():
        frequency = len(instance_list) / total
        outlier_score = 1.0 - frequency  # Rare values get high scores
        
        for instance in instance_list:
            scores[instance] = {
                'value': value,
                'frequency': frequency,
                'outlier_score': outlier_score,
                'peers_with_same_value': instance_list,
                'majority_value': max(value_counts.items(), key=lambda x: len(x[1]))[0]
            }
    
    return scores

# Example usage:
instances = {
    'SMP201': True,
    'HUB01': True,
    'CLIP01': True,
    'DEV01': False,  # Outlier
    'BENT01': True,
    # ... 15 more instances with True
}

scores = calculate_outlier_score('CMI', 'ReSpawn.Enabled', instances)
# DEV01 will have outlier_score ≈ 0.95 (very unusual)
# Others will have outlier_score ≈ 0.05 (common)
```

---

## Drift Report Format

### Standard Drift (baseline comparison):
```json
{
  "server_name": "DEV01",
  "plugin_name": "CMI",
  "config_file": "config",
  "key_path": "ReSpawn.Enabled",
  "expected_value": true,
  "actual_value": false,
  "drift_type": "value_mismatch",
  "severity": "low"
}
```

### Outlier Drift (peer comparison):
```json
{
  "server_name": "DEV01",
  "plugin_name": "CMI",
  "config_file": "config",
  "key_path": "ReSpawn.Enabled",
  "expected_value": true,
  "actual_value": false,
  "drift_type": "outlier",
  "severity": "medium",
  "outlier_metadata": {
    "outlier_score": 0.95,
    "frequency": 0.05,  # Only 5% of instances have this value
    "majority_value": true,
    "majority_count": 19,
    "minority_count": 1,
    "peers_with_same_value": ["DEV01"],
    "peers_with_majority_value": ["SMP201", "HUB01", "CLIP01", ...]
  }
}
```

---

## Severity Classification

**HIGH Severity** (likely unintentional):
- Outlier score > 0.90 (1-2 instances different from 18+)
- Security-related settings (passwords, tokens, IPs)
- Critical plugins (LuckPerms, CoreProtect, WorldGuard)

**MEDIUM Severity** (possibly intentional):
- Outlier score 0.70-0.90 (3-5 instances different)
- Non-critical settings
- Dev/test servers with known differences

**LOW Severity** (likely intentional):
- Outlier score < 0.70 (many instances have variations)
- Instance-specific settings (server names, ports, UUIDs)
- Per-server role customizations (creative vs survival)

---

## Whitelist: Expected Deviations

Some settings SHOULD differ per instance:

```yaml
expected_deviations:
  bStats:
    - serverUuid  # Always unique per instance
  Plan:
    - Server.Name  # Unique server names
    - Webserver.Port  # Different ports per instance
  Velocity:
    - bind  # Different IPs/ports
  HuskSync:
    - server-id  # Unique identifier per instance
  CMI:
    - Server.Name  # Instance display name
```

**Algorithm adjustment:**
- If setting is in whitelist, **skip outlier detection**
- Still report in drift (for baseline comparison) but **don't flag as outlier**

---

## Implementation Plan

### Phase 1: Data Collection
```python
# Collect all instance configs for a plugin
def collect_plugin_configs(plugin_name: str, instances: list) -> dict:
    """
    Returns: {
        instance_name: {
            config_file: {key: value, ...}
        }
    }
    """
    pass
```

### Phase 2: Outlier Analysis
```python
# Analyze each setting across all instances
def analyze_outliers(plugin_configs: dict) -> list:
    """
    Returns list of outlier drift items with metadata
    """
    outliers = []
    
    # Flatten configs to setting → {instance: value}
    settings_by_instance = flatten_configs(plugin_configs)
    
    for setting_key, instance_values in settings_by_instance.items():
        # Skip whitelisted settings
        if is_whitelisted(plugin_name, setting_key):
            continue
        
        # Calculate outlier scores
        scores = calculate_outlier_score(plugin_name, setting_key, instance_values)
        
        # Flag high-score outliers
        for instance, metadata in scores.items():
            if metadata['outlier_score'] > 0.70:
                outliers.append({
                    'instance': instance,
                    'plugin': plugin_name,
                    'setting': setting_key,
                    **metadata
                })
    
    return outliers
```

### Phase 3: Report Generation
```python
# Generate human-readable report
def generate_outlier_report(outliers: list) -> str:
    """
    Markdown report highlighting unusual configurations
    """
    report = "# Configuration Outlier Analysis\n\n"
    
    # Group by severity
    high_severity = [o for o in outliers if o['outlier_score'] > 0.90]
    medium_severity = [o for o in outliers if 0.70 <= o['outlier_score'] <= 0.90]
    
    report += f"## HIGH PRIORITY ({len(high_severity)} outliers)\n\n"
    for outlier in high_severity:
        report += f"### {outlier['instance']}: {outlier['plugin']}.{outlier['setting']}\n"
        report += f"- **Value**: `{outlier['value']}`\n"
        report += f"- **Majority value**: `{outlier['majority_value']}` ({outlier['majority_count']} instances)\n"
        report += f"- **Outlier score**: {outlier['outlier_score']:.2f}\n\n"
    
    return report
```

---

## Example Output

```markdown
# Configuration Outlier Analysis

Generated: 2025-11-06 14:30:00
Scanned: 20 instances, 90 plugins

## HIGH PRIORITY (5 outliers)

### DEV01: CMI.ReSpawn.Enabled
- **Current value**: `false`
- **Majority value**: `true` (19 instances)
- **Outlier score**: 0.95
- **Recommendation**: Verify if intentional for testing

### CLIP01: CoreProtect.rollback-enabled
- **Current value**: `false`
- **Majority value**: `true` (19 instances)
- **Outlier score**: 0.95
- **Recommendation**: ⚠️ CRITICAL - Should rollbacks be disabled?

### HUB01: LuckPerms.storage-method
- **Current value**: `mysql`
- **Majority value**: `h2` (18 instances)
- **Outlier score**: 0.90
- **Recommendation**: Verify database connection

## MEDIUM PRIORITY (12 outliers)

### EVO01: WorldEdit.max-blocks-changed.maximum
- **Current value**: `-1`
- **Majority value**: `1000000` (17 instances)
- **Outlier score**: 0.85
- **Recommendation**: Unlimited WorldEdit for testing?

[... more outliers ...]

## SUMMARY

- **Total outliers found**: 17
- **Plugins with most outliers**: CMI (5), LuckPerms (3), CoreProtect (2)
- **Instances with most outliers**: DEV01 (8), EVO01 (4), CLIP01 (2)

**Next steps**: Review HIGH priority outliers for unintentional configuration errors.
```

---

## Usage

```bash
# Run outlier analysis
python -m src.analyzers.outlier_detector \
  --baseline data/baselines/universal_configs \
  --instances all \
  --output reports/outliers_$(date +%Y%m%d).md

# Filter by plugin
python -m src.analyzers.outlier_detector \
  --plugin CMI \
  --min-score 0.80

# Filter by instance
python -m src.analyzers.outlier_detector \
  --instance DEV01 \
  --show-peers
```

---

## Integration with Existing Drift Detector

Add `detect_outliers()` method to `DriftDetector` class:

```python
class DriftDetector:
    def detect_outliers(self, plugin_name: str, instances: list) -> list:
        """
        Detect configuration outliers (1 instance different from rest)
        
        Args:
            plugin_name: Plugin to analyze
            instances: List of instance names to compare
            
        Returns:
            List of outlier drift items
        """
        # Implementation here
        pass
```

This complements the existing `detect_drift()` method which compares against baseline.
